{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an image classification model and deploy to Azure Arc-enabled Kubernetes Cluster\n",
    "\n",
    "In this notebook, you train a machine learning model within Azure Machine Learning. You'll use the training and deployment workflow for Azure Machine Learning service in a Python Jupyter notebook.  You can then use the notebook as a template to train your own machine learning model with your own data.  \n",
    "\n",
    "**_The main point to understand is that you can train a model in Azure ML, attach to a Kubernetes Cluster and deploy that model onto the Kubernetes Cluster._**\n",
    "\n",
    "This tutorial trains a simple logistic regression using the [MNIST](https://azure.microsoft.com/services/open-datasets/catalog/mnist/) dataset and [scikit-learn](http://scikit-learn.org) with Azure Machine Learning.  MNIST is a popular dataset consisting of 70,000 grayscale images. Each image is a handwritten digit of 28x28 pixels, representing a number from 0 to 9. The goal is to create a multi-class classifier to identify the digit a given image represents. \n",
    "\n",
    "High Level Steps:\n",
    "\n",
    "> * Import necessary libraries for authentication and Azure ML services\n",
    "> * Connect to Azure Machine Learning Workspace\n",
    "> * Grab your Training Data\n",
    "> * Create your Compute so you can train your data\n",
    "> * Configure and run the Training Job\n",
    "> * Register Model\n",
    "> * Create a Kubernetes Endpoint\n",
    "> * Deploy the model to an Azure Arc-Enabled Kubernetes Cluster Pod\n",
    "> * Test: Visualize using postman\n",
    "\n",
    "References:\n",
    "\n",
    "> * https://learn.microsoft.com/en-us/azure/machine-learning/how-to-attach-kubernetes-anywhere\n",
    "> * https://github.com/Azure/AML-Kubernetes\n",
    "> * https://medium.com/@jmasengesho/azure-machine-learning-service-for-kubernetes-architects-part-i-ml-extension-and-inference-router-2a763fb9960d\n",
    "> * https://medium.com/@jmasengesho/azure-machine-learning-service-for-kubernetes-architects-deploy-your-first-model-on-aks-with-az-440ada47b4a0\n",
    "> * https://azurearcjumpstart.com/azure_arc_jumpstart/azure_arc_ml/aks/aks_blob_mnist_arm_template\n",
    "> * https://github.com/microsoft/azure_arc/tree/main/azure_arc_ml_jumpstart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up your development environment\n",
    "\n",
    "All the setup for your development work can be accomplished in a Python notebook.  Setup includes:\n",
    "\n",
    "* Importing Python packages\n",
    "* Connecting to a workspace to enable communication between your local computer and remote resources\n",
    "* Downloading the data to train the model\n",
    "* Creating an job/experiment to track all your runs\n",
    "* Creating a Kubernetes Endpoint\n",
    "* Creating a deployment with your model\n",
    "* Deploying the model onto the Kubernetes Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for authentication and Azure ML services\n",
    "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential, AzureCliCredential\n",
    "from azure.ai.ml import automl, command, MLClient, Input, Output, VERSION\n",
    "from azure.ai.ml.entities import Job, AmlCompute, Data, Environment, Model, CodeConfiguration, KubernetesOnlineEndpoint, KubernetesOnlineDeployment, OnlineRequestSettings, OnlineScaleSettings, ResourceSettings, ResourceRequirementsSettings\n",
    "from azure.ai.ml.constants import AssetTypes\n",
    "\n",
    "# Additional Python standard libraries\n",
    "import os\n",
    "\n",
    "# Import AutoML primary metrics for various tasks\n",
    "from azure.ai.ml.automl import (\n",
    "    ClassificationPrimaryMetrics,           # For multi-class image classification\n",
    "    ClassificationMultilabelPrimaryMetrics, # For multi-label image classification\n",
    "    ObjectDetectionPrimaryMetrics,          # For object detection tasks\n",
    "    InstanceSegmentationPrimaryMetrics      # For instance segmentation tasks\n",
    ")\n",
    "\n",
    "from azure.ai.ml.entities._deployment.resource_requirements_settings import ResourceRequirementsSettings\n",
    "from azure.ai.ml.entities._deployment.container_resource_settings import ResourceSettings\n",
    "\n",
    "# Print the Azure AI ML SDK version\n",
    "print ('azure.ai.ml: ' + VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "load workspace"
    ]
   },
   "outputs": [],
   "source": [
    "# Connect to a workspace\n",
    "try:\n",
    "    credential = DefaultAzureCredential()\n",
    "    # Check if given credential can get token successfully.\n",
    "    credential.get_token(\"https://management.azure.com/.default\")\n",
    "except Exception as ex:\n",
    "    # Fall back to InteractiveBrowserCredential in case DefaultAzureCredential not work\n",
    "    credential = InteractiveBrowserCredential()\n",
    "\n",
    "ml_client = MLClient.from_config(credential)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore data\n",
    "\n",
    "Before you train a model, you need to understand the data that you are using to train it. In this section you learn how to:\n",
    "\n",
    "* Download the MNIST dataset\n",
    "* Display some sample images\n",
    "\n",
    "### Download the MNIST dataset\n",
    "\n",
    "Use Azure Open Datasets to get the raw MNIST data files. [Azure Open Datasets](https://docs.microsoft.com/azure/open-datasets/overview-what-are-open-datasets) are curated public datasets that you can use to add scenario-specific features to machine learning solutions for more accurate models. Each dataset has a corrseponding class, `MNIST` in this case, to retrieve the data in different ways.\n",
    "\n",
    "This code retrieves the data as a `FileDataset` object, which is a subclass of `Dataset`. A `FileDataset` references single or multiple files of any format in your datastores or public urls. The class provides you with the ability to download or mount the files to your compute by creating a reference to the data source location. Additionally, you register the Dataset to your workspace for easy retrieval during training.\n",
    "\n",
    "Follow the [how-to](https://aka.ms/azureml/howto/createdatasets) to learn more about Datasets and their usage in the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "# Define the data folder and ensure it exists\n",
    "data_folder = os.path.join(os.getcwd(), 'data', 'sklearn')\n",
    "os.makedirs(data_folder, exist_ok=True)\n",
    "\n",
    "# Define the dataset using the Azure Open Datasets for MNIST\n",
    "mnist_data_url = \"https://azureopendatastorage.blob.core.windows.net/datasets/mnist.zip\"\n",
    "\n",
    "# Download the MNIST dataset\n",
    "# https://learn.microsoft.com/en-us/azure/open-datasets/dataset-mnist\n",
    "urllib.request.urlretrieve(\"https://azuremlexamples.blob.core.windows.net/datasets/mnist/train-images-idx3-ubyte.gz\", \n",
    "                           os.path.join(data_folder, \"train-images-idx3-ubyte.gz\"))\n",
    "urllib.request.urlretrieve(\"https://azuremlexamples.blob.core.windows.net/datasets/mnist/train-labels-idx1-ubyte.gz\", \n",
    "                           os.path.join(data_folder, \"train-labels-idx1-ubyte.gz\"))\n",
    "urllib.request.urlretrieve(\"https://azuremlexamples.blob.core.windows.net/datasets/mnist/t10k-images-idx3-ubyte.gz\", \n",
    "                           os.path.join(data_folder, \"t10k-images-idx3-ubyte.gz\"))\n",
    "urllib.request.urlretrieve(\"https://azuremlexamples.blob.core.windows.net/datasets/mnist/t10k-labels-idx1-ubyte.gz\", \n",
    "                           os.path.join(data_folder, \"t10k-labels-idx1-ubyte.gz\"))\n",
    "\n",
    "# Register the dataset to the workspace\n",
    "mnist_dataset = Data(\n",
    "    name=\"mnist_opendataset\",\n",
    "    path=data_folder,  # Path where MNIST data is stored\n",
    "    type=\"uri_folder\",  # The data is stored as a folder\n",
    "    description=\"Training and test dataset for MNIST\",\n",
    ")\n",
    "\n",
    "# Create or update the dataset in Azure ML\n",
    "ml_client.data.create_or_update(mnist_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some sample images\n",
    "\n",
    "Load the compressed files into `numpy` arrays. Then use `matplotlib` to plot 30 random images from the dataset with their labels above them. Note this step requires a `load_data` function that's included in an `utils.py` file. This file is included in the sample folder. Please make sure it is placed in the same folder as this notebook. The `load_data` function simply parses the compresse files into numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure utils.py is in the same directory as this code\n",
    "from utils import load_data\n",
    "import glob\n",
    "\n",
    "# note we also shrink the intensity values (X) from 0-255 to 0-1. This helps the model converge faster.\n",
    "X_train = load_data(glob.glob(os.path.join(data_folder,\"**/train-images-idx3-ubyte.gz\"), recursive=True)[0], False) / 255.0\n",
    "X_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-images-idx3-ubyte.gz\"), recursive=True)[0], False) / 255.0\n",
    "y_train = load_data(glob.glob(os.path.join(data_folder,\"**/train-labels-idx1-ubyte.gz\"), recursive=True)[0], True).reshape(-1)\n",
    "y_test = load_data(glob.glob(os.path.join(data_folder,\"**/t10k-labels-idx1-ubyte.gz\"), recursive=True)[0], True).reshape(-1)\n",
    "\n",
    "# now let's show some randomly chosen images from the traininng set.\n",
    "count = 0\n",
    "sample_size = 30\n",
    "plt.figure(figsize = (16, 6))\n",
    "for i in np.random.permutation(X_train.shape[0])[:sample_size]:\n",
    "    count = count + 1\n",
    "    plt.subplot(1, sample_size, count)\n",
    "    plt.axhline('')\n",
    "    plt.axvline('')\n",
    "    plt.text(x=10, y=-10, s=y_train[i], fontsize=18)\n",
    "    plt.imshow(X_train[i].reshape(28, 28), cmap=plt.cm.Greys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on Azure ML Cluster or Serverless Compute\n",
    "\n",
    "For this task, you submit the job to run on the Azure ML cluster or Serverless Compute.  To submit a job you:\n",
    "* Create a directory\n",
    "* Create a training script\n",
    "* Create Compute\n",
    "* Configure the training job \n",
    "* Submit the job "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a directory to deliver the necessary code from your computer to the remote resource.\n",
    "import os\n",
    "script_folder = os.path.join(os.getcwd(), \"sklearn-mnist\")\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a training script\n",
    "\n",
    "To submit the job to the cluster, first create a training script. Run the following code to create the training script called `train.py` in the directory you just created. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile $script_folder/train.py\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "from azureml.core import Run\n",
    "from utils import load_data\n",
    "\n",
    "# let user feed in 2 parameters, the dataset to mount or download, and the regularization rate of the logistic regression model\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--data-folder', type=str, dest='data_folder', help='data folder mounting point')\n",
    "parser.add_argument('--regularization', type=float, dest='reg', default=0.01, help='regularization rate')\n",
    "args = parser.parse_args()\n",
    "\n",
    "data_folder = args.data_folder\n",
    "print('Data folder:', data_folder)\n",
    "\n",
    "# load train and test set into numpy arrays\n",
    "# note we scale the pixel intensity values to 0-1 (by dividing it with 255.0) so the model can converge faster.\n",
    "X_train = load_data(glob.glob(os.path.join(data_folder, '**/train-images-idx3-ubyte.gz'), recursive=True)[0], False) / 255.0\n",
    "X_test = load_data(glob.glob(os.path.join(data_folder, '**/t10k-images-idx3-ubyte.gz'), recursive=True)[0], False) / 255.0\n",
    "y_train = load_data(glob.glob(os.path.join(data_folder, '**/train-labels-idx1-ubyte.gz'), recursive=True)[0], True).reshape(-1)\n",
    "y_test = load_data(glob.glob(os.path.join(data_folder, '**/t10k-labels-idx1-ubyte.gz'), recursive=True)[0], True).reshape(-1)\n",
    "\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape, sep = '\\n')\n",
    "\n",
    "# get hold of the current run\n",
    "run = Run.get_context()\n",
    "\n",
    "print('Train a logistic regression model with regularization rate of', args.reg)\n",
    "clf = LogisticRegression(C=1.0/args.reg, solver=\"liblinear\", multi_class=\"auto\", random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "print('Predict the test set')\n",
    "y_hat = clf.predict(X_test)\n",
    "\n",
    "# calculate accuracy on the prediction\n",
    "acc = np.average(y_hat == y_test)\n",
    "print('Accuracy is', acc)\n",
    "\n",
    "run.log('regularization rate', np.float(args.reg))\n",
    "run.log('accuracy', np.float(acc))\n",
    "\n",
    "os.makedirs('outputs', exist_ok=True)\n",
    "# note file saved in the outputs folder is automatically uploaded into experiment record\n",
    "joblib.dump(value=clf, filename='outputs/sklearn_mnist_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notice how the script gets data and saves models:\n",
    "\n",
    "+ The training script reads an argument to find the directory containing the data.  When you submit the job later, you point to the dataset for this argument:\n",
    "`parser.add_argument('--data-folder', type=str, dest='data_folder', help='data directory mounting point')`\n",
    "\n",
    "+ The training script saves your model into a directory named outputs. <br/>\n",
    "`joblib.dump(value=clf, filename='outputs/sklearn_mnist_model.pkl')`<br/>\n",
    "Anything written in this directory is automatically uploaded into your workspace. You'll access your model from this directory later in the tutorial.\n",
    "\n",
    "The file `utils.py` is referenced from the training script to load the dataset correctly.  Copy this script into the script folder so that it can be accessed along with the training script on the remote resource."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "shutil.copy('utils.py', script_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some compute so you can train your model on this compute cluster\n",
    "cpu_compute_target = \"aml-cluster\"\n",
    "\n",
    "try:\n",
    "    ml_client.compute.get(cpu_compute_target)\n",
    "except Exception:\n",
    "    print(\"Creating a new cpu compute target...\")\n",
    "    compute = AmlCompute(\n",
    "        name=cpu_compute_target, size=\"Standard_DS3_v2\", min_instances=0, max_instances=4\n",
    "    )\n",
    "    ml_client.compute.begin_create_or_update(compute).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure the training job\n",
    "\n",
    "Create a Job Command object to specify the configuration details of your training job, including your training script, environment to use, and the compute target to run on. Configure the Job by specifying:\n",
    "\n",
    "* The directory that contains your scripts. All the files in this directory are uploaded into the cluster nodes for execution. \n",
    "* The compute target.  In this case you will use serverless compute.\n",
    "* The training script name, train.py\n",
    "* An environment that contains the libraries needed to run the script\n",
    "* Arguments required from the training script. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Training Job\n",
    "experiment_name = 'aio-ai-sklearn-mnist-exp'\n",
    "\n",
    "job = command(\n",
    "    code=script_folder,  # Path to your training scripts folder: sklearn-mnist-scripts\n",
    "    # environment=\"AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest\",\n",
    "    environment=\"azureml://registries/azureml/environments/sklearn-1.5/labels/latest\",\n",
    "    command=\"python train.py --data-folder ${{inputs.data_folder}} --regularization ${{inputs.regularization}}\",\n",
    "    inputs={\"data_folder\": Input(type=\"uri_folder\", path=data_folder), \"regularization\": 0.5},\n",
    "    #outputs={\"model_output\": Output(type=\"uri_folder\", path=\"./outputs\")},\n",
    "    # compute=cpu_compute_target,  # If you dont add compute it will default to serverless\n",
    "    description=\"AIO AI - Train MNIST Job\",\n",
    "    experiment_name=experiment_name,\n",
    "    tags={\"foo\": \"bar\"}\n",
    ")\n",
    "\n",
    "# job.set_limits(\n",
    "#     max_trials=10,\n",
    "#     max_concurrent_trials=2,\n",
    "#     timeout=3600, # Set timeout to 1 hour (3600 seconds)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Submit the job\n",
    "returned_job = ml_client.jobs.create_or_update(job)\n",
    "print(f\"Submitted job: {job}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stream the output and wait until the job is finished\n",
    "ml_client.jobs.stream(returned_job.name)\n",
    "\n",
    "# Refresh the latest status of the job after streaming\n",
    "returned_job = ml_client.jobs.get(name=returned_job.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get metrics after job completion\n",
    "print(returned_job.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if returned_job.status == \"Completed\":\n",
    "    # lets get the model from this run\n",
    "    model = Model(\n",
    "        # the train.py script stores the model as \"sklearn_mnist_model\"\n",
    "        path=f\"azureml://jobs/{returned_job.name}/outputs/artifacts/outputs/sklearn_mnist_model.pkl\",\n",
    "        name=\"sklearn_mnist\",\n",
    "        description=\"AIO AI MNIST model trained using Scikit-learn.\",\n",
    "        type=\"custom_model\",\n",
    "        tags={\"framework\": \"scikit-learn\", \"dataset\": \"MNIST\"}\n",
    "    )\n",
    "    print(\"Job Status: {}.\".format(returned_job.status))\n",
    "else:\n",
    "    print(\"Job Status: {}. Please wait until it completes\".format(returned_job.status))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Register this model\n",
    "ml_client.models.create_or_update(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create local folder and download the model to a folder named artifact_downloads/outputs\n",
    "local_dir = \"./artifact_downloads/outputs\"\n",
    "if not os.path.exists(local_dir):\n",
    "    os.mkdir(local_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example if providing an specific Job name/ID\n",
    "# job_name = \"salmon_camel_5sdf05xvb3\"\n",
    "\n",
    "# Get the parent run\n",
    "mlflow_parent_run = ml_client.jobs.get(job_name)\n",
    "\n",
    "print(\"Parent Run: \")\n",
    "print(mlflow_parent_run)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download model to artifacts/outputs\n",
    "ml_client.jobs.download(name=job_name, output_name=\"outputs\", download_path=local_dir)\n",
    "\n",
    "print(f\"Artifacts downloaded in: {local_dir}\")\n",
    "print(f\"Artifacts: {os.listdir(local_dir)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a unique endpoint name with current datetime to avoid conflicts\n",
    "import datetime\n",
    "\n",
    "online_endpoint_name = \"k8s-endpoint-\" + datetime.datetime.now().strftime(\"%m%d%H%M%f\")\n",
    "print(online_endpoint_name)\n",
    "\n",
    "# Create an online endpoint\n",
    "# To serve the online endpoint in Kubernetes, set the compute as your Kubernetes compute target.\n",
    "endpoint = KubernetesOnlineEndpoint(\n",
    "    name=online_endpoint_name,\n",
    "    compute=\"k3s-cluster\",\n",
    "    description=\"SkLearn Kubernetes realtime endpoint.\",\n",
    "    auth_mode=\"key\",\n",
    "    tags={\"modelName\": \"sklearn-mnist\"}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the online endpoint\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
    "print(f\"Created endpoint: {online_endpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a depolyment\n",
    "model = Model(path=\"./model-1/model/sklearn_regression_model.pkl\")\n",
    "env = Environment(\n",
    "    conda_file=\"./model-1/environment/conda.yaml\",\n",
    "    image=\"mcr.microsoft.com/azureml/openmpi4.1.0-ubuntu20.04\",\n",
    ")\n",
    "\n",
    "requests = ResourceSettings(cpu=\"0.1\", memory=\"0.2G\")\n",
    "limits = ResourceSettings(cpu=\"0.2\", memory=\"0.5G\")\n",
    "resources = ResourceRequirementsSettings(requests=requests, limits=limits)\n",
    "\n",
    "blue_deployment = KubernetesOnlineDeployment(\n",
    "    name=\"blue\",\n",
    "    endpoint_name=online_endpoint_name,\n",
    "    model=model,\n",
    "    environment=env,\n",
    "    code_configuration=CodeConfiguration(\n",
    "        code=\"./model-1/onlinescoring\", scoring_script=\"score.py\"\n",
    "    ),\n",
    "    instance_count=1,\n",
    "    resources=resources,\n",
    "    request_settings=OnlineRequestSettings(\n",
    "        request_timeout_ms=30000,\n",
    "        max_queue_wait_ms=30000\n",
    "    ),\n",
    "    scale_settings=OnlineScaleSettings(\n",
    "        type=\"target_utilization\",\n",
    "        min_instances=1,\n",
    "        max_instances=3,\n",
    "        polling_interval=10,\n",
    "        target_utilization_percentage=70\n",
    "    ),\n",
    "    instance_type=\"defaultinstancetype\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the 'blue' deployment to the endpoint\n",
    "ml_client.online_deployments.begin_create_or_update(blue_deployment).result()\n",
    "print(\"Created 'blue' deployment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Traffic Distribution Between 'blue' and 'green' Deployments\n",
    "# Set the traffic routing to distribute 70% traffic to 'blue' and 30% to 'green'\n",
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "endpoint.traffic = {\n",
    "    \"blue\": 100  # 70% traffic goes to 'blue' deployment\n",
    "    #\"green\": 30  # 30% traffic goes to 'green' deployment\n",
    "}\n",
    "\n",
    "# Update the endpoint to apply the traffic distribution\n",
    "ml_client.online_endpoints.begin_create_or_update(endpoint).result()\n",
    "# print(\"Traffic distribution updated: 70% to 'blue', 30% to 'green'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the details for online endpoint\n",
    "endpoint = ml_client.online_endpoints.get(name=online_endpoint_name)\n",
    "print(f\"Endpoint Name: {endpoint.name}\")\n",
    "#print(endpoint)\n",
    "\n",
    "# existing traffic details\n",
    "print(endpoint.traffic)\n",
    "\n",
    "# Get the scoring URI\n",
    "print(endpoint.scoring_uri)\n",
    "\n",
    "# Get the details for the deployment\n",
    "deployment = ml_client.online_deployments.get(endpoint_name=online_endpoint_name, name='blue')\n",
    "print(f\"Deployment Name: {deployment.name}\")\n",
    "print(f\"Deployment Scale Settings: {deployment.scale_settings}\")\n",
    "print(f\"Deployment Resource Requirements: {deployment.resources}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_client.online_deployments.get_logs(\n",
    "    name=\"blue\", endpoint_name=online_endpoint_name, lines=50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test the blue deployment with some sample data\n",
    "# comment this out as cluster under dev subscription can't be accessed from public internet.\n",
    "ml_client.online_endpoints.invoke(\n",
    "   endpoint_name=online_endpoint_name,\n",
    "   deployment_name='blue',\n",
    "   request_file='./model-1/sample-request.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Delete the endpoint\n",
    "ml_client.online_endpoints.begin_delete(name=online_endpoint_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curl -v -i -X POST -H \"Content-Type:application/json\" -H \"Authorization: Bearer <key_or_token>\" -d '<sample_data>' <scoring_uri>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "\n",
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n",
    "\n",
    "data = {}\n",
    "\n",
    "body = str.encode(json.dumps(data))\n",
    "\n",
    "url = 'http://10.7.0.4/api/v1/endpoint/k8s-endpoint-09101145643044/score'\n",
    "\n",
    "api_key = '1CI5eN7bw0Pp7wcfY7KXn4o8E5WApQ1J'\n",
    "\n",
    "headers = {'Content-Type':'application/json', 'Authorization':('Bearer '+ api_key)}\n",
    "\n",
    "req = urllib.request.Request(url, body, headers)\n",
    "\n",
    "try:\n",
    "    response = urllib.request.urlopen(req)\n",
    "\n",
    "    result = response.read()\n",
    "    print(result)\n",
    "except urllib.error.HTTPError as error:\n",
    "    print(\"The request failed with status code: \" + str(error.code))\n",
    "\n",
    "    # Print the headers - they include the requert ID and the timestamp, which are useful for debugging the failure\n",
    "    print(error.info())\n",
    "    print(error.read().decode(\"utf8\", 'ignore'))"
   ]
  }
 ],
 "metadata": {
  "authors": [
   {
    "name": "maxluk"
   }
  ],
  "categories": [
   "tutorials",
   "image-classification-mnist-data"
  ],
  "kernelspec": {
   "display_name": "Python (amlarc)",
   "language": "python",
   "name": "amlarc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "msauthor": "roastala",
  "network_required": false
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
